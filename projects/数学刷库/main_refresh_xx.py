"""
æ•°å­¦å•æ¨¡è§†é¢‘æ¸…æ´— (Refactored with checkpoint_to_file)
"""
import json
import re
import argparse
from pathlib import Path
import time
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import os
import sys

common_utils_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "../..")
sys.path.append(common_utils_path)

# å¼•å…¥ä½ çš„å·¥å…·åº“
from common_utils import read_jsonl, get_values_by_key_path, checkpoint_to_file, run_pipeline

import logging
logger = logging.getLogger(__name__)

# ==================== åŸºç¡€å¤„ç†é€»è¾‘ (ä¿æŒä¸å˜) ====================

def read_init_data(data_path):
    yield from read_jsonl(data_path)

def filter_empty_video(samples, tran_script_key, phase):
    """è¿‡æ»¤æ— è§†é¢‘æˆ–ç»“æ„ä¸æ­£ç¡®çš„æ•°æ®"""
    for sample in samples:
        tran_scripts = get_values_by_key_path(sample, tran_script_key)
        if len(tran_scripts) != 1:
            continue
        tran_script = tran_scripts[0]
        try:
            if isinstance(tran_script, str):
                tran_script = json.loads(tran_script)
            else:
                assert isinstance(tran_script, dict)
            
            struct = tran_script.get("scienceStruct", {})
            required_keys = ["content", "readQuestion", "conclusion"]
            
            if phase == "åˆä¸­":
                required_keys.extend(["analyses", "standardAnswers"])
            elif phase == "å°å­¦":
                required_keys.extend(["analyse", "standardAnswer"])
            else:
                continue

            if not all(k in struct for k in required_keys):
                continue
                
        except:
            continue
        yield sample

def format_input(samples, tran_script_key, phase):
    """å°†åŸå§‹æ•°æ®æ ¼å¼åŒ–ä¸ºå¾…å¤„ç†æ ¼å¼"""
    for sample in samples:
        tran_script = get_values_by_key_path(sample, tran_script_key)[0]
        if isinstance(tran_script, str):
            tran_script = json.loads(tran_script)
            
        struct = tran_script["scienceStruct"]
        
        if phase == "åˆä¸­":
            video_cont = struct["content"]
            video_read = struct["readQuestion"]
            video_analyses = struct["analyses"]
            video_answers = struct["standardAnswers"]
            video_conclusion = struct["conclusion"]
        elif phase == "å°å­¦":
            video_cont = struct["content"]
            video_read = struct["readQuestion"]
            video_analyses = [struct["analyse"]]
            video_answers = [struct["standardAnswer"]]
            video_conclusion = struct["conclusion"]
        else:
            continue

        result = {}
        result["è¯»é¢˜"] = {"è®²è§£å†…å®¹": video_read.get("explainContent", "")}
        
        for i, (ana, ans) in enumerate(zip(video_analyses, video_answers)):
            suffix = "" if len(video_analyses) == 1 else str(i + 1)
            result[f"åˆ†æ{suffix}"] = {
                "è®²è§£å†…å®¹": ana.get("explainContent", ""), 
                "å±•ç¤ºå†…å®¹": ana.get("displayContent", "")
            }
            result[f"æ ‡å‡†ä½œç­”{suffix}"] = {
                "è®²è§£å†…å®¹": ans.get("explainContent", ""), 
                "å±•ç¤ºå†…å®¹": ans.get("displayContent", "")
            }
            
        result["æ€»ç»“"] = {
            "è®²è§£å†…å®¹": video_conclusion.get("explainContent", ""), 
            "å±•ç¤ºå†…å®¹": video_conclusion.get("displayContent", "")
        }
        
        final_result = {
            "id": sample["topic_id"], 
            "video_question": video_cont, 
            "video": result
        }
        yield final_result

def to_crawl_in(samples, prompt_path):
    """ç”Ÿæˆçˆ¬è™«è¾“å…¥Prompt"""
    with open(prompt_path) as reader:
        prompt_template = reader.read().strip()
    for sample in samples:
        prompt = prompt_template.replace("{{ques}}", sample["video_question"]).replace("{{video}}", json.dumps(sample["video"], ensure_ascii=False, indent=2))
        sample["query"] = prompt
        yield sample

def filter_video_logic(samples):
    """å¤„ç†è§†é¢‘æ‹’è¯†é€»è¾‘"""
    for sample in samples:
        answer = sample.get("answer", "")
        if '"å®¡æ ¸ç»“æœ": "ä¸åˆæ ¼"' in answer:
            result = {
                "topic_id": sample["id"], 
                "tran_script_version": "å•æ¨¡è§†é¢‘è¿‡æ»¤", 
                "tran_script_operate_type": "null"
            }
            yield result

# --- Network Utils ---

def create_retry_session(retries=3, backoff_factor=0.5, timeout=30):
    retry_strategy = Retry(
        total=retries,
        backoff_factor=backoff_factor,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["POST"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session = requests.Session()
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.timeout = timeout
    session.headers.update({
        'User-Agent': 'Mozilla/5.0',
        'Content-Type': 'application/json',
        'Connection': 'keep-alive'
    })
    return session

def safe_post_request(url, payload, max_attempts=5):
    session = create_retry_session(retries=3, timeout=60)
    for attempt in range(max_attempts):
        try:
            response = session.post(url, json=payload)
            response.raise_for_status()
            return response
        except Exception as e:
            time.sleep(2 ** attempt)
            if attempt == max_attempts - 1:
                print(f"Request failed: {e}")
    return None

def filter_latex_logic(samples):
    """Latex è¿‡æ»¤é€»è¾‘"""
    url = "http://localhost:3000/latex2html"
    for sample in samples:
        assert "id" in sample
        is_latex_good = True
        question = sample["video_question"]
        video = sample["video"]
        video_display = [question]
        
        for k in video:
            content = video[k].get("å±•ç¤ºå†…å®¹", "")
            if isinstance(content, dict):
                video_display.extend(list(content.keys()))
            elif isinstance(content, list):
                video_display.extend(content)
            elif isinstance(content, str):
                video_display.append(content)

        # ç®€å•çš„æ¸…æ´—å’Œæ£€æŸ¥
        processed_display = []
        for c in video_display:
            if not isinstance(c, str): continue
            # ç®€å•çš„æ›¿æ¢é€»è¾‘ (ä¿ç•™åŸæ¥çš„)
            c = c.replace("\tau", "\\tau").replace("\triangle", "\\triangle") \
                 .replace("\times", "\\times").replace("\therefore", "\\therefore") \
                 .replace("\text", "\\text").replace("\neg", "\\neg") \
                 .replace("\neq", "\\neq").replace("\nabl", "\\nabl") \
                 .replace("\newline", "\\newline").replace("\t", "").replace("\n", "").replace("\"", "'")
            
            c_dump = json.dumps(c, ensure_ascii=False)
            if ("\\" in c_dump or "^" in c_dump) and "$" not in c:
                is_latex_good = False
                break
            processed_display.append(c)

        if not is_latex_good:
            yield {"topic_id": sample["id"], "tran_script_version": "å•æ¨¡è§†é¢‘latexè¿‡æ»¤", "tran_script_operate_type": "null"}
            continue
            
        video_display_str = "\n".join(processed_display)
        if re.search(r'[a-zA-Z]', video_display_str) is None:
            continue
            
        payload = {"text": video_display_str}
        response = safe_post_request(url, payload)
        
        if response and response.status_code == 200:
            data = response.json()
            if data.get("code") != 200:
                is_latex_good = False
        else:
             # å¦‚æœæœåŠ¡ä¸é€šï¼Œç­–ç•¥æ˜¯ä¿ç•™è¿˜æ˜¯è¿‡æ»¤ï¼Ÿæ­¤å¤„ä¿æŒåŸé€»è¾‘(è®¤ä¸ºæ˜¯åçš„?)
             # åŸé€»è¾‘é‡Œå¦‚æœæœªå“åº”ä¼¼ä¹æ²¡æœ‰ç½®ä¸ºFalseï¼Œè¿™é‡Œéœ€æ³¨æ„
             pass 

        if not is_latex_good:
            yield {"topic_id": sample["id"], "tran_script_version": "å•æ¨¡è§†é¢‘latexè¿‡æ»¤", "tran_script_operate_type": "null"}

def filter_analysis_logic(samples):
    for sample in samples:
        if sample.get("analysis_operate_type") in ["delete", "null"]:
            yield {"topic_id": sample["topic_id"], "tran_script_version": "å•æ¨¡è§†é¢‘è§£æè¿‡æ»¤ç»§æ‰¿", "tran_script_operate_type": "null"}

def filter_all_logic(samples_orig, samples_video, samples_analysis, samples_latex):
    id2result = {}
    # ä¼˜å…ˆçº§è¦†ç›–ï¼šlatex > analysis > video (æ ¹æ®åŸé€»è¾‘é¡ºåº)
    # åŸé€»è¾‘æ˜¯å…ˆéå†latex, analysis, videoæ”¾å…¥å­—å…¸ï¼Œåæ”¾å…¥çš„ä¼šè¦†ç›–å‰é¢çš„
    # å‡è®¾ä¸€ä¸ªIDåªä¼šåœ¨ä¸€ç§è¿‡æ»¤ä¸­å‡ºç°é—®é¢˜ï¼Œæˆ–è€…å–å¹¶é›†
    
    for l in samples_latex: id2result[l["topic_id"]] = l
    for l in samples_analysis: id2result[l["topic_id"]] = l
    for l in samples_video: id2result[l["topic_id"]] = l
    
    for sample in samples_orig:
        tid = sample.get("topic_id")
        if not tid: continue
        
        if tid in id2result:
            yield id2result[tid]
        else:
            yield {"topic_id": tid, "tran_script_version": "å•æ¨¡è§†é¢‘è¿‡æ»¤", "tran_script_operate_type": "same"}


# ==================== æ ¸å¿ƒä¿®æ”¹ï¼šPipeline å‡½æ•° (ä½¿ç”¨ @checkpoint_to_file) ====================

@checkpoint_to_file
def pipeline_crawl_input(orig_data_path, tran_script_key, phase, prompt_path):
    """
    é˜¶æ®µ1ï¼šç”Ÿæˆçˆ¬å–è¾“å…¥
    è¯»å– -> è¿‡æ»¤ç©ºè§†é¢‘ -> æ ¼å¼åŒ– -> ç”ŸæˆPrompt
    """
    samples = read_init_data(orig_data_path)
    samples = filter_empty_video(samples, tran_script_key, phase)
    samples = format_input(samples, tran_script_key, phase)
    yield from to_crawl_in(samples, prompt_path)

@checkpoint_to_file
def pipeline_crawl_output_filter(crawl_out_path):
    """
    é˜¶æ®µ2ï¼šå¤„ç†çˆ¬å–è¾“å‡º
    è¯»å–çˆ¬è™«ç»“æœ -> è¿‡æ»¤ä¸åˆæ ¼
    """
    # è¿™é‡Œç›´æ¥è¯»æ–‡ä»¶å³å¯ï¼Œread_jsonlåœ¨common_utilsé‡Œ
    samples = read_jsonl(crawl_out_path) 
    yield from filter_video_logic(samples)

@checkpoint_to_file
def pipeline_latex_filter(orig_data_path, tran_script_key, phase):
    """
    é˜¶æ®µ3ï¼šLatex è¿‡æ»¤
    è¯»å– -> æ ¼å¼åŒ– -> è¯·æ±‚æœåŠ¡è¿‡æ»¤
    """
    samples = read_init_data(orig_data_path)
    samples = filter_empty_video(samples, tran_script_key, phase)
    samples = format_input(samples, tran_script_key, phase)
    yield from filter_latex_logic(samples)

@checkpoint_to_file
def pipeline_analysis_filter(analysis_result_paths):
    """
    é˜¶æ®µ4ï¼šè§£æè¿‡æ»¤ç»§æ‰¿
    """
    # æ”¯æŒå¤šä¸ªè·¯å¾„pattern
    samples = read_jsonl(analysis_result_paths)
    yield from filter_analysis_logic(samples)

@checkpoint_to_file
def pipeline_combine_all(orig_data_path, video_res_path, analysis_res_path, latex_res_path):
    """
    é˜¶æ®µ5ï¼šç»¼åˆæ‰€æœ‰ç»“æœ
    æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æŠŠå‰é¢æ­¥éª¤ç”Ÿæˆçš„æ–‡ä»¶è¯»è¿›æ¥
    """
    # 1. åŸå§‹æ•°æ® (ä½œä¸ºåŸºå‡†æµ)
    samples_orig = read_init_data(orig_data_path)
    
    # 2. è¯»å–å„é˜¶æ®µçš„ä¸­é—´ç»“æœ (ListåŒ–ä»¥ä¾¿æŸ¥æ‰¾ï¼Œå¦‚æœæ•°æ®é‡å·¨å¤§éœ€ä¼˜åŒ–é€»è¾‘)
    # å› ä¸ºè¦æ„å»º id2result å­—å…¸ï¼Œå¿…é¡»å…ˆåŠ è½½åˆ°å†…å­˜
    samples_video = list(read_jsonl(video_res_path))
    samples_analysis = list(read_jsonl(analysis_res_path))
    samples_latex = list(read_jsonl(latex_res_path))
    
    yield from filter_all_logic(samples_orig, samples_video, samples_analysis, samples_latex)


# ==================== Main ====================

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    parser = argparse.ArgumentParser()
    parser.add_argument("--phase", type=str, choices=["å°å­¦", "åˆä¸­"], default="åˆä¸­")
    parser.add_argument("--part", type=str, default="1")
    parser.add_argument("--tran_script_key", "--tkey", type=str, default=".tran_script[].tranScript")
    parser.add_argument("--prompt_path", "--ppath", type=str, default="/mnt/pan8T/temp_djguo/å­˜é‡åº“æ¸…æ´—-åˆä¸­å•æ¨¡/æ•ˆæœéªŒè¯/å°åˆå•æ¨¡è§†é¢‘é€å­—ç¨¿+è¯»é¢˜ç­›é€‰prompt_P1127.md")
    parser.add_argument("--save_dir", "--sdir", type=str)
    parser.add_argument("--stage", type=str, required=True, 
                        choices=["ä¿å­˜çˆ¬å–è¾“å…¥", "çˆ¬å–è¾“å‡ºç»“æœ", "latexè¿‡æ»¤ç»“æœ", "è§£æè¿‡æ»¤ç»“æœ", "ç»¼åˆè¿‡æ»¤ç»“æœ"])
    parser.add_argument("--orig_data_path", type=str)
    # æ”¯æŒ list å‚æ•°
    parser.add_argument("--analysis_result_path", type=str, nargs="+")
    
    args = parser.parse_args()
    # æ‰“å°å‚æ•°
    logger.info("=" * 50)
    logger.info(f"ğŸš€ ä»»åŠ¡å¯åŠ¨: [Part {args.part}] - {args.phase}")
    logger.info(f"ğŸ“Œ å½“å‰é˜¶æ®µ: {args.stage}")
    logger.info(f"ğŸ“‚ åŸå§‹æ•°æ®: {args.orig_data_path}")
    logger.info(f"ğŸ“‚ ä¿å­˜ç›®å½•: {args.save_dir}")
    logger.info("=" * 50)

    # é¢„å®šä¹‰æ–‡ä»¶è·¯å¾„è§„åˆ™ (é›†ä¸­ç®¡ç†è·¯å¾„ï¼Œé¿å…æ•£è½åœ¨å„å¤„)
    path_crawl_in = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘è´¨é‡è¿‡æ»¤çˆ¬å–è¾“å…¥", f"{args.phase}_å•æ¨¡_part{args.part}_video_check_crawl_in.json").as_posix()
    
    # å‡è®¾çˆ¬å–è¾“å‡ºçš„æ–‡ä»¶åè§„åˆ™ï¼ˆé€šå¸¸æ˜¯è¾“å…¥æ–‡ä»¶åæ”¹ä¸€ä¸‹åç¼€ï¼Œæˆ–è€…æ˜¯äººå·¥æŒ‡å®šï¼‰
    # è¿™é‡Œå‡è®¾è¾“å…¥æ–‡ä»¶è·‘å®Œæ¨¡å‹åï¼Œæ”¾åœ¨ "çˆ¬å–è¾“å‡º" ç›®å½•
    path_crawl_out = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘è´¨é‡è¿‡æ»¤çˆ¬å–è¾“å‡º", f"{args.phase}_å•æ¨¡_part{args.part}_video_check_crawl_out.json").as_posix()
    
    path_video_res = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘è´¨é‡è¿‡æ»¤ç»“æœ", f"{args.phase}_å•æ¨¡_part{args.part}_video_check_result.json").as_posix()
    path_latex_res = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘latexè¿‡æ»¤ç»“æœ", f"{args.phase}_å•æ¨¡_part{args.part}_latex_check_result.json").as_posix()
    path_analysis_res = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘è§£æè¿‡æ»¤ç»§æ‰¿ç»“æœ", f"{args.phase}_å•æ¨¡_analysis_check_result.json").as_posix()
    path_final_res = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘ç»¼åˆè¿‡æ»¤ç»“æœ", f"part{args.part}_video_check_result.json").as_posix()
    
    # ==================== æ‰§è¡Œé€»è¾‘ ====================
    # ä½¿ç”¨æ–¹å¼ï¼š func(save_path=..., mode="write")(ä¸šåŠ¡å‚æ•°...)

    if args.stage == "ä¿å­˜çˆ¬å–è¾“å…¥":
        logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_crawl_in}")
        run_pipeline(
            pipeline_crawl_input(
                save_path=path_crawl_in, 
                mode="write", 
                overwrite=True  # é€šå¸¸ç”Ÿæˆè¾“å…¥æ˜¯ç¬¬ä¸€æ­¥ï¼Œå¯ä»¥è¦†ç›–
            )(
                orig_data_path=args.orig_data_path,
                tran_script_key=args.tran_script_key,
                phase=args.phase,
                prompt_path=args.prompt_path
            )
        )

    elif args.stage == "çˆ¬å–è¾“å‡ºç»“æœ":
        logger.info(f"ğŸ”™ è¾“å…¥çˆ¬è™«ç»“æœ: {path_crawl_out}")
        logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_video_res}")
        run_pipeline(
            pipeline_crawl_output_filter(
                save_path=path_video_res,
                mode="write"
            )(
                crawl_out_path=path_crawl_out
            )
        )

    elif args.stage == "latexè¿‡æ»¤ç»“æœ":
        logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_latex_res}")
        run_pipeline(
            pipeline_latex_filter(
                save_path=path_latex_res,
                mode="write"
            )(
                orig_data_path=args.orig_data_path,
                tran_script_key=args.tran_script_key,
                phase=args.phase
            )
       )
    elif args.stage == "è§£æè¿‡æ»¤ç»“æœ":
        logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_analysis_res}")
        run_pipeline(
            pipeline_analysis_filter(
                save_path=path_analysis_res,
                mode="write"
            )(
                analysis_result_paths=args.analysis_result_path
            )
        )

    elif args.stage == "ç»¼åˆè¿‡æ»¤ç»“æœ":
        logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_final_res}")
        # è¿™ä¸€æ­¥ä¾èµ–å‰é¢çš„ç»“æœæ–‡ä»¶å­˜åœ¨
        run_pipeline(
            pipeline_combine_all(
                save_path=path_final_res,
                mode="write"
            )(
                orig_data_path=args.orig_data_path,
                video_res_path=path_video_res,
                analysis_res_path=path_analysis_res,
                latex_res_path=path_latex_res
            )
        )