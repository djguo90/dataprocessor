"""
æ•°å­¦å•æ¨¡é‡åˆ·è§†é¢‘æ¸…æ´— (Refactored with checkpoint_to_file)
1. è§£ææ ‡ç­¾ï¼šnullï¼šåˆ é™¤
2. 

"""
import json
import re
import argparse
from pathlib import Path
import time
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import os
import sys

common_utils_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "../..")
sys.path.append(common_utils_path)

# å¼•å…¥ä½ çš„å·¥å…·åº“
from common_utils import read_jsonl, get_values_by_key_path, checkpoint_to_file, run_pipeline

import logging
logger = logging.getLogger(__name__)


# è·å–æŒ‡å®šç±»åˆ«çš„æ•°æ®
def get_specific_data_ids(samples_orig, samples_question, samples_answer_analysis, question_type, analysis_type):
    """
    é¢˜å¹²sameæˆ–delete: åˆ™ä½¿ç”¨åŸç»“æœ
    é¢˜å¹²update: 
    """
    # print(question_type)
    # print(analysis_type)
    idx_question = set()
    for sample in samples_question:
        # print(sample["content_available_operate_type"])
        if sample["content_available_operate_type"] == question_type:
            idx_question.add(sample["topic_id"])
        # else:
        #     print(sample["content_available_operate_type"])
    idx_analysis = set()
    for sample in samples_answer_analysis:
        # print(sample["analysis_operate_type"])
        if sample["analysis_operate_type"] == analysis_type:
            idx_analysis.add(sample["topic_id"])
    # print(len(idx_question))
    # print(len(idx_analysis))
    for sample in samples_orig:
        if sample["topic_id"]  in idx_question and sample["topic_id"] in idx_analysis:
            yield sample


def read_init_data(data_path):
    yield from read_jsonl(data_path)

def filter_empty_video(samples, tran_script_key, phase):
    """è¿‡æ»¤æ— è§†é¢‘æˆ–ç»“æ„ä¸æ­£ç¡®çš„æ•°æ®"""
    for sample in samples:
        tran_scripts = get_values_by_key_path(sample, tran_script_key)
        if len(tran_scripts) != 1:
            continue
        tran_script = tran_scripts[0]
        try:
            if isinstance(tran_script, str):
                tran_script = json.loads(tran_script)
            else:
                assert isinstance(tran_script, dict)
            
            struct = tran_script.get("scienceStruct", {})
            required_keys = ["content", "readQuestion", "conclusion"]
            
            if phase == "åˆä¸­":
                required_keys.extend(["analyses", "standardAnswers"])
            elif phase == "å°å­¦":
                required_keys.extend(["analyse", "standardAnswer"])
            else:
                continue

            if not all(k in struct for k in required_keys):
                continue
                
        except:
            continue
        yield sample

def format_input(samples, tran_script_key, phase):
    """å°†åŸå§‹æ•°æ®æ ¼å¼åŒ–ä¸ºå¾…å¤„ç†æ ¼å¼"""
    for sample in samples:
        tran_script = get_values_by_key_path(sample, tran_script_key)[0]
        if isinstance(tran_script, str):
            tran_script = json.loads(tran_script)
            
        struct = tran_script["scienceStruct"]
        
        if phase == "åˆä¸­":
            video_cont = struct["content"]
            video_read = struct["readQuestion"]
            video_analyses = struct["analyses"]
            video_answers = struct["standardAnswers"]
            video_conclusion = struct["conclusion"]
        elif phase == "å°å­¦":
            video_cont = struct["content"]
            video_read = struct["readQuestion"]
            video_analyses = [struct["analyse"]]
            video_answers = [struct["standardAnswer"]]
            video_conclusion = struct["conclusion"]
        else:
            continue

        result = {}
        result["è¯»é¢˜"] = {"è®²è§£å†…å®¹": video_read.get("explainContent", "")}
        
        for i, (ana, ans) in enumerate(zip(video_analyses, video_answers)):
            suffix = "" if len(video_analyses) == 1 else str(i + 1)
            result[f"åˆ†æ{suffix}"] = {
                "è®²è§£å†…å®¹": ana.get("explainContent", ""), 
                "å±•ç¤ºå†…å®¹": ana.get("displayContent", "")
            }
            result[f"æ ‡å‡†ä½œç­”{suffix}"] = {
                "è®²è§£å†…å®¹": ans.get("explainContent", ""), 
                "å±•ç¤ºå†…å®¹": ans.get("displayContent", "")
            }
            
        result["æ€»ç»“"] = {
            "è®²è§£å†…å®¹": video_conclusion.get("explainContent", ""), 
            "å±•ç¤ºå†…å®¹": video_conclusion.get("displayContent", "")
        }
        
        final_result = {
            "id": sample["topic_id"], 
            "video_question": video_cont, 
            "video": result
        }
        yield final_result

def to_crawl_in(samples, prompt_path):
    """ç”Ÿæˆçˆ¬è™«è¾“å…¥Prompt"""
    with open(prompt_path) as reader:
        prompt_template = reader.read().strip()
    for sample in samples:
        prompt = prompt_template.replace("{{ques}}", sample["video_question"]).replace("{{video}}", json.dumps(sample["video"], ensure_ascii=False, indent=2))
        sample["query"] = prompt
        yield sample

def filter_video_logic(samples):
    """å¤„ç†è§†é¢‘æ‹’è¯†é€»è¾‘"""
    for sample in samples:
        answer = sample.get("answer", "")
        if '"å®¡æ ¸ç»“æœ": "ä¸åˆæ ¼"' in answer:
            result = {
                "topic_id": sample["id"], 
                "tran_script_version": "å•æ¨¡è§†é¢‘è¿‡æ»¤", 
                "tran_script_operate_type": "null"
            }
            yield result

# --- Network Utils ---

def create_retry_session(retries=3, backoff_factor=0.5, timeout=30):
    retry_strategy = Retry(
        total=retries,
        backoff_factor=backoff_factor,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["POST"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session = requests.Session()
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.timeout = timeout
    session.headers.update({
        'User-Agent': 'Mozilla/5.0',
        'Content-Type': 'application/json',
        'Connection': 'keep-alive'
    })
    return session

def safe_post_request(url, payload, max_attempts=5):
    session = create_retry_session(retries=3, timeout=60)
    for attempt in range(max_attempts):
        try:
            response = session.post(url, json=payload)
            response.raise_for_status()
            return response
        except Exception as e:
            time.sleep(2 ** attempt)
            if attempt == max_attempts - 1:
                print(f"Request failed: {e}")
    return None

def filter_latex_logic(samples):
    """Latex è¿‡æ»¤é€»è¾‘"""
    url = "http://localhost:3000/latex2html"
    for sample in samples:
        assert "id" in sample
        is_latex_good = True
        question = sample["video_question"]
        video = sample["video"]
        video_display = [question]
        
        for k in video:
            content = video[k].get("å±•ç¤ºå†…å®¹", "")
            if isinstance(content, dict):
                video_display.extend(list(content.keys()))
            elif isinstance(content, list):
                video_display.extend(content)
            elif isinstance(content, str):
                video_display.append(content)

        # ç®€å•çš„æ¸…æ´—å’Œæ£€æŸ¥
        processed_display = []
        for c in video_display:
            if not isinstance(c, str): continue
            # ç®€å•çš„æ›¿æ¢é€»è¾‘ (ä¿ç•™åŸæ¥çš„)
            c = c.replace("\tau", "\\tau").replace("\triangle", "\\triangle") \
                 .replace("\times", "\\times").replace("\therefore", "\\therefore") \
                 .replace("\text", "\\text").replace("\neg", "\\neg") \
                 .replace("\neq", "\\neq").replace("\nabl", "\\nabl") \
                 .replace("\newline", "\\newline").replace("\t", "").replace("\n", "").replace("\"", "'")
            
            c_dump = json.dumps(c, ensure_ascii=False)
            if ("\\" in c_dump or "^" in c_dump) and "$" not in c:
                is_latex_good = False
                break
            processed_display.append(c)

        if not is_latex_good:
            yield {"topic_id": sample["id"], "tran_script_version": "å•æ¨¡è§†é¢‘latexè¿‡æ»¤", "tran_script_operate_type": "null"}
            continue
            
        video_display_str = "\n".join(processed_display)
        if re.search(r'[a-zA-Z]', video_display_str) is None:
            continue
            
        payload = {"text": video_display_str}
        response = safe_post_request(url, payload)
        
        if response and response.status_code == 200:
            data = response.json()
            if data.get("code") != 200:
                is_latex_good = False
        else:
             # å¦‚æœæœåŠ¡ä¸é€šï¼Œç­–ç•¥æ˜¯ä¿ç•™è¿˜æ˜¯è¿‡æ»¤ï¼Ÿæ­¤å¤„ä¿æŒåŸé€»è¾‘(è®¤ä¸ºæ˜¯åçš„?)
             # åŸé€»è¾‘é‡Œå¦‚æœæœªå“åº”ä¼¼ä¹æ²¡æœ‰ç½®ä¸ºFalseï¼Œè¿™é‡Œéœ€æ³¨æ„
             pass 

        if not is_latex_good:
            yield {"topic_id": sample["id"], "tran_script_version": "å•æ¨¡è§†é¢‘latexè¿‡æ»¤", "tran_script_operate_type": "null"}

def filter_analysis_logic(samples):
    for sample in samples:
        if sample.get("analysis_operate_type") in ["delete", "null"]:
            yield {"topic_id": sample["topic_id"], "tran_script_version": "å•æ¨¡è§†é¢‘è§£æè¿‡æ»¤ç»§æ‰¿", "tran_script_operate_type": "null"}

def filter_all_logic(samples_orig, samples_video, samples_analysis, samples_latex):
    id2result = {}
    # ä¼˜å…ˆçº§è¦†ç›–ï¼šlatex > analysis > video (æ ¹æ®åŸé€»è¾‘é¡ºåº)
    # åŸé€»è¾‘æ˜¯å…ˆéå†latex, analysis, videoæ”¾å…¥å­—å…¸ï¼Œåæ”¾å…¥çš„ä¼šè¦†ç›–å‰é¢çš„
    # å‡è®¾ä¸€ä¸ªIDåªä¼šåœ¨ä¸€ç§è¿‡æ»¤ä¸­å‡ºç°é—®é¢˜ï¼Œæˆ–è€…å–å¹¶é›†
    
    for l in samples_latex: id2result[l["topic_id"]] = l
    for l in samples_analysis: id2result[l["topic_id"]] = l
    for l in samples_video: id2result[l["topic_id"]] = l
    
    for sample in samples_orig:
        tid = sample.get("topic_id")
        if not tid: continue
        
        if tid in id2result:
            yield id2result[tid]
        else:
            yield {"topic_id": tid, "tran_script_version": "å•æ¨¡è§†é¢‘è¿‡æ»¤", "tran_script_operate_type": "same"}


# ==================== æ ¸å¿ƒä¿®æ”¹ï¼šPipeline å‡½æ•° (ä½¿ç”¨ @checkpoint_to_file) ====================

# @checkpoint_to_file
# def pipeline_crawl_input(orig_data_path, tran_script_key, phase, prompt_path):
#     """
#     é˜¶æ®µ1ï¼šç”Ÿæˆçˆ¬å–è¾“å…¥
#     è¯»å– -> è¿‡æ»¤ç©ºè§†é¢‘ -> æ ¼å¼åŒ– -> ç”ŸæˆPrompt
#     """
#     samples = read_init_data(orig_data_path)
#     samples = filter_empty_video(samples, tran_script_key, phase)
#     samples = format_input(samples, tran_script_key, phase)
#     yield from to_crawl_in(samples, prompt_path)

# @checkpoint_to_file
# def pipeline_crawl_output_filter(crawl_out_path):
#     """
#     é˜¶æ®µ2ï¼šå¤„ç†çˆ¬å–è¾“å‡º
#     è¯»å–çˆ¬è™«ç»“æœ -> è¿‡æ»¤ä¸åˆæ ¼
#     """
#     # è¿™é‡Œç›´æ¥è¯»æ–‡ä»¶å³å¯ï¼Œread_jsonlåœ¨common_utilsé‡Œ
#     samples = read_jsonl(crawl_out_path) 
#     yield from filter_video_logic(samples)

# @checkpoint_to_file
# def pipeline_latex_filter(orig_data_path, tran_script_key, phase):
#     """
#     é˜¶æ®µ3ï¼šLatex è¿‡æ»¤
#     è¯»å– -> æ ¼å¼åŒ– -> è¯·æ±‚æœåŠ¡è¿‡æ»¤
#     """
#     samples = read_init_data(orig_data_path)
#     samples = filter_empty_video(samples, tran_script_key, phase)
#     samples = format_input(samples, tran_script_key, phase)
#     yield from filter_latex_logic(samples)

# @checkpoint_to_file
# def pipeline_analysis_filter(analysis_result_paths):
#     """
#     é˜¶æ®µ4ï¼šè§£æè¿‡æ»¤ç»§æ‰¿
#     """
#     # æ”¯æŒå¤šä¸ªè·¯å¾„pattern
#     samples = read_jsonl(analysis_result_paths)
#     yield from filter_analysis_logic(samples)

# @checkpoint_to_file
# def pipeline_combine_all(orig_data_path, video_res_path, analysis_res_path, latex_res_path):
#     """
#     é˜¶æ®µ5ï¼šç»¼åˆæ‰€æœ‰ç»“æœ
#     æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æŠŠå‰é¢æ­¥éª¤ç”Ÿæˆçš„æ–‡ä»¶è¯»è¿›æ¥
#     """
#     # 1. åŸå§‹æ•°æ® (ä½œä¸ºåŸºå‡†æµ)
#     samples_orig = read_init_data(orig_data_path)
    
#     # 2. è¯»å–å„é˜¶æ®µçš„ä¸­é—´ç»“æœ (ListåŒ–ä»¥ä¾¿æŸ¥æ‰¾ï¼Œå¦‚æœæ•°æ®é‡å·¨å¤§éœ€ä¼˜åŒ–é€»è¾‘)
#     # å› ä¸ºè¦æ„å»º id2result å­—å…¸ï¼Œå¿…é¡»å…ˆåŠ è½½åˆ°å†…å­˜
#     samples_video = list(read_jsonl(video_res_path))
#     samples_analysis = list(read_jsonl(analysis_res_path))
#     samples_latex = list(read_jsonl(latex_res_path))
    
#     yield from filter_all_logic(samples_orig, samples_video, samples_analysis, samples_latex)


# ==================== Main ====================

if __name__ == "__main__":
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    parser = argparse.ArgumentParser()
    parser.add_argument("--phase", type=str, default="å°å­¦", choices=["å°å­¦", "åˆä¸­"])
    parser.add_argument("--part", type=str, default="å†’çƒŸ")
    parser.add_argument("--video-check-prompt_path", "--ppath", type=str, default="/mnt/pan8T/temp_djguo/å­˜é‡åº“æ¸…æ´—-åˆä¸­å•æ¨¡/æ•ˆæœéªŒè¯/å°åˆå•æ¨¡è§†é¢‘é€å­—ç¨¿+è¯»é¢˜ç­›é€‰prompt_P1127.md")
    parser.add_argument("--save_dir", "--sdir", type=str)
    parser.add_argument("--stage", type=str, choices=["æŸ¥çœ‹åˆ†å¸ƒ", "çˆ¬å–è¾“å…¥"], default="æŸ¥çœ‹åˆ†å¸ƒ")
    args = parser.parse_args()

    if args.part == "å†’çƒŸ":
        # åŸå§‹æ•°æ®
        orig_data_path = "/mnt/onet/temp_mypeng2/data/1224_data_process/test_nums-10000.json"
        # é¢˜å¹²æ£€æµ‹ç»“æœ
        question_result_path = "/mnt/pan8T/temp_zyhu22/xiaoshu_repair/1w/repair_1w.json"
        # ç­”æ¡ˆè§£ææ£€æµ‹ç»“æœ
        analysis_result_path = "/mnt/pan8T/temp_jiahe3/results/primary_repair_query/final_result/1w/primary_repair_analysis_answer_1w.json"
        # æ›´æ–°åçš„è§†é¢‘
        video_update_result_path = "/train31/bdr/permanent/lqkang/topic_source_data/0302_MISO/jiaofu_all/qiangsun9/primary_repair_trans_1w.json"
        # æ›´æ–°åçš„é¢„ç½®é—®é¢˜
        preset_question_update_result_path = "/train31/bdr/permanent/lqkang/topic_source_data/0302_MISO/jiaofu_all/qiangsun9/primary_repair_preset_question_1w.json"
    
    # é¢„å®šä¹‰æ–‡ä»¶è·¯å¾„è§„åˆ™ (é›†ä¸­ç®¡ç†è·¯å¾„ï¼Œé¿å…æ•£è½åœ¨å„å¤„)
    path_crawl_in = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘è´¨é‡è¿‡æ»¤çˆ¬å–è¾“å…¥", f"{args.phase}_å•æ¨¡_part{args.part}_video_check_crawl_in.json").as_posix()
    path_crawl_out = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘è´¨é‡è¿‡æ»¤çˆ¬å–è¾“å‡º", f"{args.phase}_å•æ¨¡_part{args.part}_video_check_crawl_out.json").as_posix()
    path_video_res = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘è´¨é‡è¿‡æ»¤ç»“æœ", f"{args.phase}_å•æ¨¡_part{args.part}_video_check_result.json").as_posix()
    path_latex_res = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘latexè¿‡æ»¤ç»“æœ", f"{args.phase}_å•æ¨¡_part{args.part}_latex_check_result.json").as_posix()
    path_analysis_res = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘è§£æè¿‡æ»¤ç»§æ‰¿ç»“æœ", f"{args.phase}_å•æ¨¡_analysis_check_result.json").as_posix()
    path_final_res = Path(f"{args.save_dir}", f"{args.phase}å•æ¨¡è§†é¢‘ç»¼åˆè¿‡æ»¤ç»“æœ", f"part{args.part}_video_check_result.json").as_posix()
    # æ‰“å°å‚æ•°
    logger.info("=" * 50)
    logger.info(f"ğŸš€ ä»»åŠ¡å¯åŠ¨: [Part {args.part}] - {args.phase}")
    logger.info(f"ğŸ“Œ å½“å‰é˜¶æ®µ: {args.stage}")
    logger.info(f"ğŸ“‚ åŸå§‹é¢˜åº“æ•°æ®: {orig_data_path}")
    logger.info(f"ğŸ“‚ é¢˜å¹²æ£€æµ‹æ•°æ®: {question_result_path}")
    logger.info(f"ğŸ“‚ ç­”æ¡ˆè§£ææ£€æµ‹æ•°æ®: {analysis_result_path}")
    logger.info(f"ğŸ“‚ ä¿å­˜ç›®å½•: {args.save_dir}")
    logger.info("=" * 50)
    # å–éœ€è¦é‡æ–°åˆ¤æ–­çš„æ•°æ®
    if args.stage == "æŸ¥çœ‹åˆ†å¸ƒ":
        for q_type in ["same", "update", "delete"]:
            for a_type in ["same", "update", "null"]:
                samples_orig = read_jsonl(orig_data_path)
                samples_question_result = read_jsonl(question_result_path)
                samples_analysis_result = read_jsonl(analysis_result_path)
                samples = get_specific_data_ids(samples_orig, samples_question_result, samples_analysis_result, q_type, a_type)
                logger.info(f"-"*50)
                logger.info(f"é¢˜å¹²æ“ä½œç±»å‹: {q_type}")
                logger.info(f"è§£ææ“ä½œç±»å‹: {a_type}")
                logger.info(f"è¯•é¢˜æ•°é‡: {len(list(samples))}")
                # print(q_type, a_type, len(list(samples)))

    
    # # ==================== æ‰§è¡Œé€»è¾‘ ====================
    # # ä½¿ç”¨æ–¹å¼ï¼š func(save_path=..., mode="write")(ä¸šåŠ¡å‚æ•°...)

    # if args.stage == "ä¿å­˜çˆ¬å–è¾“å…¥":
    #     logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_crawl_in}")
    #     run_pipeline(
    #         pipeline_crawl_input(
    #             save_path=path_crawl_in, 
    #             mode="write", 
    #             overwrite=True  # é€šå¸¸ç”Ÿæˆè¾“å…¥æ˜¯ç¬¬ä¸€æ­¥ï¼Œå¯ä»¥è¦†ç›–
    #         )(
    #             orig_data_path=args.orig_data_path,
    #             tran_script_key=args.tran_script_key,
    #             phase=args.phase,
    #             prompt_path=args.prompt_path
    #         )
    #     )

    # elif args.stage == "çˆ¬å–è¾“å‡ºç»“æœ":
    #     logger.info(f"ğŸ”™ è¾“å…¥çˆ¬è™«ç»“æœ: {path_crawl_out}")
    #     logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_video_res}")
    #     run_pipeline(
    #         pipeline_crawl_output_filter(
    #             save_path=path_video_res,
    #             mode="write"
    #         )(
    #             crawl_out_path=path_crawl_out
    #         )
    #     )

    # elif args.stage == "latexè¿‡æ»¤ç»“æœ":
    #     logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_latex_res}")
    #     run_pipeline(
    #         pipeline_latex_filter(
    #             save_path=path_latex_res,
    #             mode="write"
    #         )(
    #             orig_data_path=args.orig_data_path,
    #             tran_script_key=args.tran_script_key,
    #             phase=args.phase
    #         )
    #    )
    # elif args.stage == "è§£æè¿‡æ»¤ç»“æœ":
    #     logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_analysis_res}")
    #     run_pipeline(
    #         pipeline_analysis_filter(
    #             save_path=path_analysis_res,
    #             mode="write"
    #         )(
    #             analysis_result_paths=args.analysis_result_path
    #         )
    #     )

    # elif args.stage == "ç»¼åˆè¿‡æ»¤ç»“æœ":
    #     logger.info(f"ğŸ”œ ç›®æ ‡è¾“å‡ºè·¯å¾„: {path_final_res}")
    #     # è¿™ä¸€æ­¥ä¾èµ–å‰é¢çš„ç»“æœæ–‡ä»¶å­˜åœ¨
    #     run_pipeline(
    #         pipeline_combine_all(
    #             save_path=path_final_res,
    #             mode="write"
    #         )(
    #             orig_data_path=args.orig_data_path,
    #             video_res_path=path_video_res,
    #             analysis_res_path=path_analysis_res,
    #             latex_res_path=path_latex_res
    #         )
    #     )